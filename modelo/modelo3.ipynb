{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df1f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50 # type: ignore\n",
    "from tensorflow.keras import layers, models # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping # type: ignore\n",
    "from tensorflow.keras.metrics import AUC, F1Score # type: ignore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array # type: ignore\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n",
    "from tensorflow.keras import backend as K # type: ignore\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "from tensorflow.keras.regularizers import l2 # type: ignore\n",
    "from tensorflow.keras.models import load_model  # type: ignore\n",
    "from tensorflow.keras.callbacks import TensorBoard # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7623c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "    def load_data(self):\n",
    "        for label in ['plaga', 'sana']:\n",
    "            label_dir = os.path.join(self.data_dir, label)\n",
    "            for image_name in os.listdir(label_dir):\n",
    "                img_path = os.path.join(label_dir, image_name)\n",
    "                img_array = self.load_and_preprocess_image(img_path)\n",
    "                if img_array is not None:\n",
    "                    self.images.append(img_array)\n",
    "                    self.labels.append(self.assign_label(label))\n",
    "\n",
    "    def load_and_preprocess_image(self, img_path):\n",
    "        try:\n",
    "            img = load_img(img_path, target_size=(224, 224))  # Redimensionar\n",
    "            img_array = img_to_array(img) / 255.0  # Normalizar\n",
    "            return img_array\n",
    "        except OSError as e:\n",
    "            print(f\"Error al cargar la imagen {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def assign_label(self, label):\n",
    "        return 1 if label == 'plaga' else 0  # Etiquetas binarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a0e84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetModel:\n",
    "    def __init__(self):\n",
    "        \"\"\"Usamos ResNet50 como base y agregamos capas de salida.\"\"\"\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "\n",
    "        for layer in base_model.layers[-20:]:\n",
    "            layer.trainable = True#Congelar las capas de la base para evitar el sobreajuste\n",
    "        \n",
    "        \n",
    "        self.model = models.Sequential([\n",
    "            base_model,\n",
    "            layers.GlobalAveragePooling2D(),  # Promedio global de las características\n",
    "            layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "            layers.Dense(1, activation='sigmoid')  # Salida binaria (plaga o sana)\n",
    "        ])\n",
    "        \n",
    "    \n",
    "\n",
    "    def compile(self):\n",
    "        \"\"\"Compila el modelo.\"\"\"\n",
    "        \n",
    "        lr_schedule = ExponentialDecay(\n",
    "            initial_learning_rate=0.001, #Valor inicial del learning rate\n",
    "            decay_steps=50000,  # Número de pasos antes de reducir el learning rate\n",
    "            decay_rate=0.96,     # Factor de reducción en cada paso\n",
    "            staircase=True       # Si se debe hacer el descenso en \"escalones\"\n",
    "        )\n",
    "        \n",
    "        optimizer = Adam(learning_rate=lr_schedule) # Usamos Adam con un scheduler de decaimiento exponencial\n",
    "        \n",
    "        self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', AUC(), F1Score()]) \n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=6, batch_size=32):\n",
    "        \"\"\"Entrena el modelo con data augmentation.\"\"\" \n",
    "\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "        print(\"Iniciando el entrenamiento del modelo...\")\n",
    "        history = self.model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[early_stopping, tensorboard_callback],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "\n",
    "\n",
    "    def evaluate(self, X_val, y_val):\n",
    "        \"\"\"Evalúa el modelo con los datos de validación.\"\"\"\n",
    "        loss, accuracy, auc, f1 = self.model.evaluate(X_val, y_val)\n",
    "        print(f'Pérdida: {loss}, Accuracy: {accuracy}, AUC: {auc}, F1-Score: {f1}')\n",
    "        return loss, accuracy, auc, f1\n",
    "\n",
    "    def save(self, model_name='modelo_entrenado_resnet.h5'):\n",
    "        \"\"\"Guarda el modelo entrenado.\"\"\"\n",
    "        self.model.save(model_name)\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f83ca70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2466, 1)\n",
      "(617, 1)\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/'  # Cambia a la ruta de tus imágenes\n",
    "dataset = ImageDataset(data_dir)\n",
    "dataset.load_data()\n",
    "\n",
    "# Convertir listas a arrays de NumPy\n",
    "images = np.array(dataset.images)\n",
    "labels = np.array(dataset.labels)\n",
    "\n",
    "\n",
    "# Dividir las imágenes en entrenamiento (80%) y validación (20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Agregar una dimensión adicional para la metrica de f1-score \n",
    "y_train  = np.expand_dims(y_train, axis=-1)\n",
    "y_val = np.expand_dims(y_val, axis=-1) \n",
    "\n",
    "#print(\"Clases en train:\", np.bincount(y_train))\n",
    "#print(\"Clases en val:\", np.bincount(y_val))\n",
    "\n",
    "print(y_train.shape)  # → (2466,)\n",
    "print(y_val.shape)    # → (617,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86368f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilando el modelo...\n",
      "Iniciando el entrenamiento del modelo...\n",
      "Epoch 1/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m827s\u001b[0m 10s/step - accuracy: 0.9199 - auc_6: 0.9675 - f1_score: 0.6863 - loss: 0.5501 - val_accuracy: 0.4684 - val_auc_6: 0.5000 - val_f1_score: 0.0000e+00 - val_loss: 396.3120\n",
      "Epoch 2/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 10s/step - accuracy: 0.9236 - auc_6: 0.9687 - f1_score: 0.6537 - loss: 0.4983 - val_accuracy: 0.4684 - val_auc_6: 0.4602 - val_f1_score: 0.6866 - val_loss: 7.4578\n",
      "Epoch 3/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m747s\u001b[0m 10s/step - accuracy: 0.9239 - auc_6: 0.9686 - f1_score: 0.6659 - loss: 0.4295 - val_accuracy: 0.5316 - val_auc_6: 0.5000 - val_f1_score: 0.6942 - val_loss: 8.5540\n",
      "Epoch 4/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m727s\u001b[0m 9s/step - accuracy: 0.9500 - auc_6: 0.9870 - f1_score: 0.6802 - loss: 0.3031 - val_accuracy: 0.5316 - val_auc_6: 0.8884 - val_f1_score: 0.6942 - val_loss: 2.1750\n",
      "Epoch 5/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1140s\u001b[0m 15s/step - accuracy: 0.9186 - auc_6: 0.9708 - f1_score: 0.6921 - loss: 0.3447 - val_accuracy: 0.5316 - val_auc_6: 0.3471 - val_f1_score: 0.6942 - val_loss: 9.5473\n",
      "Epoch 6/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1354s\u001b[0m 17s/step - accuracy: 0.9331 - auc_6: 0.9867 - f1_score: 0.6824 - loss: 0.2516 - val_accuracy: 0.8104 - val_auc_6: 0.8383 - val_f1_score: 0.6942 - val_loss: 0.7687\n",
      "Epoch 7/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1278s\u001b[0m 16s/step - accuracy: 0.9604 - auc_6: 0.9936 - f1_score: 0.6755 - loss: 0.1816 - val_accuracy: 0.5316 - val_auc_6: 0.8207 - val_f1_score: 0.6942 - val_loss: 2.2254\n",
      "Epoch 8/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1216s\u001b[0m 16s/step - accuracy: 0.9514 - auc_6: 0.9843 - f1_score: 0.6691 - loss: 0.2339 - val_accuracy: 0.5316 - val_auc_6: 0.5225 - val_f1_score: 0.6942 - val_loss: 4.3907\n",
      "Epoch 9/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1251s\u001b[0m 16s/step - accuracy: 0.9476 - auc_6: 0.9850 - f1_score: 0.6737 - loss: 0.2285 - val_accuracy: 0.5316 - val_auc_6: 0.5296 - val_f1_score: 0.6942 - val_loss: 5.3087\n",
      "Epoch 10/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1256s\u001b[0m 16s/step - accuracy: 0.9558 - auc_6: 0.9915 - f1_score: 0.6811 - loss: 0.1774 - val_accuracy: 0.5316 - val_auc_6: 0.7588 - val_f1_score: 0.6942 - val_loss: 2.8405\n",
      "Epoch 11/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1251s\u001b[0m 16s/step - accuracy: 0.9702 - auc_6: 0.9929 - f1_score: 0.6763 - loss: 0.1501 - val_accuracy: 0.5332 - val_auc_6: 0.6282 - val_f1_score: 0.6942 - val_loss: 1.5236\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - accuracy: 0.8034 - auc_6: 0.8399 - f1_score: 0.6896 - loss: 0.7690\n",
      "Pérdida: 0.7686914205551147, Accuracy: 0.8103727698326111, AUC: 0.8383038640022278, F1-Score: 0.6941798329353333\n"
     ]
    }
   ],
   "source": [
    "# Crear y entrenar el modelo\n",
    "model = ResNetModel()\n",
    "model.compile()\n",
    "print(\"Compilando el modelo...\")\n",
    "history = model.train(X_train, y_train, X_val, y_val, epochs=100)\n",
    "\n",
    "# Evaluar el modelo\n",
    "loss, accuracy, auc, f1 = model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e81a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener predicciones como probabilidades\n",
    "y_pred_probs = model.model.predict(X_val)\n",
    "\n",
    "# Convertir probabilidades a etiquetas binarias\n",
    "y_pred_classes = (y_pred_probs >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = np.expand_dims(y_val, axis=-1)  # → (617, 1)\n",
    "y_val_flat = y_val.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_val_flat, y_pred_classes)\n",
    "\n",
    "# Visualización\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Sano', 'Plaga'], yticklabels=['Sano', 'Plaga'])\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.show()\n",
    "\n",
    "# Reporte detallado\n",
    "print(classification_report(y_val_flat, y_pred_classes, target_names=['Sano', 'Plaga']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
